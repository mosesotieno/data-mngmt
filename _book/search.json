[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-tutorial-about",
    "href": "index.html#what-is-this-tutorial-about",
    "title": "Data Management",
    "section": "What is this tutorial about?",
    "text": "What is this tutorial about?\nThis tutorial provides a comprehensive guide to building a data management pipeline using various programming languages and tools. It aims at equiping you with the knowledge and skills necessary to efficiently download, manage, generate reports and share data outputs, particularly in the context of survey data.\nYou will learn how to leverage R for data manipulation and reporting, utilize Python for scripting and automation, and manage databases with MySQL. Additionally, the tutorial will introduce you to Batch scripting for task automation in Windows environments.\nA key focus will be on Survey Solutions, a robust platform for data collection, where you will gain familiarity with its API for downloading and managing survey data. By working with the ssaw Python package, you will learn how to integrate these tools to create a seamless data management pipeline.\nOverall, this tutorial is designed for individuals who have a basic understanding of programming and are looking to enhance their skills in data management and analysis through practical, hands-on experience.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Management",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this tutorial, check whether you have a basic understanding of the following:\nSurvey Solutions\n\nBasic understanding of the Survey Solutions platform.\nFamiliarity with its API for downloading and managing survey data.\nAbility to work with the ssaw Python package for Survey Solutions integration.\n\nPython\n\nExperience with scripting and automation.\nKnowledge of using libraries in python.\n\nR\n\nFamiliarity with data manipulation and reporting.\nComfortable working with data frames and R scripts.\nAbility to use packages for data wrangling.\n\nMySQL\n\nUnderstanding of database management and querying.\nAbility to create databases and write SQL queries.\nComfortable managing data in MySQL environments.\n\nBatch Scripting:\n\nFamiliarity with automating tasks in Windows.\nAbility to write and run batch files for task automation.\n\n\n\n\n\n\n\nNote\n\n\n\nWhile proficiency is not required in any of these areas, a good level of familiarity will be beneficial for following the tutorial. Links to deepen understanding of the tools will be provided.\n\n\nAccess the book’s code on GitHub here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Data management involves a series of meticulous practices that ensure data is well-organized, easily accessible, understandable, and preserved over time(Briney, n.d.). Effective data management reduces the risk of losing valuable information and increases its usability during and after the completion of a project. By adopting these small practices, researchers and professionals safeguard one of the most crucial outputs of the research process—the data itself. This process can significantly benefit from automation, which minimizes manual tasks, enhances consistency, and saves time, allowing for seamless data handling even long after the project’s end.\nThis tutorial is about automating data management and report writing. The growing need for timely, accurate, and efficient data processing has made automation not just a luxury but a necessity in modern day data management practices. This tutorial will take you on a step-by-step journey to streamline your data management processes by leveraging four powerful tools: Batch scripting, Python, SQL, and R.\nSurvey Solutions, a widely-used survey data collection platform, offers rich capabilities for collecting complex datasets. However, the true power lies in how quickly and effectively you can automate the retrieval, cleaning, and analysis of this data. This tutorial aims to bridge that gap. Whether you’re managing data for health research, social impact projects, or other sectors, automating the extraction and transformation of data will save hours of manual work and reduce the risk of human error.\nIn this tutorial, we will walk you through creating a robust data management pipeline, starting with Python to download data from the Survey Solutions via API. We will then transition to R to perform in-depth data management and generate dynamic reports. After the reports have been generated will get back to Python then share the reports with the stake-holders. We will then use SQL to track the progress of report sharing. Batch scripts will be used in automating all these processes. By the end of this guide, you will have a seamless workflow that not only automates your data processes but also enhances their accuracy, reliability and scalability.\n\n\n\n\nBriney, K. n.d. Data Management for Researchers: Organize, Maintain and Share Your Data for Research Success. Pelagic Publishing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data-download.html",
    "href": "data-download.html",
    "title": "2  Data Download",
    "section": "",
    "text": "2.1 Introduction\nIn the ever-evolving landscape of data collection, Survey Solutions stands out as a powerful and versatile tool designed to streamline the process. Developed by the World Bank, this innovative platform enables researchers and organizations to gather high-quality data efficiently and effectively. With its user-friendly interface and robust features, Survey Solutions empowers field staff to conduct surveys, manage complex questionnaires, and ensure data integrity in real-time.\nOne of the standout capabilities of Survey Solutions is its robust API, which allows for seamless integration with other systems and applications. This feature enables users to automate data extraction processes, enhance data management, and facilitate real-time query generation and resolution, making it easier to incorporate Survey Solutions into existing workflows.\nBy harnessing the capabilities of Survey Solutions, users can customize surveys to meet specific research needs, collect data through mobile devices, and utilize advanced tools for monitoring and data management. This flexibility not only enhances the quality of data collected but also accelerates decision-making processes in various sectors, including healthcare, education, and social research.\nWhether you’re a seasoned researcher or a novice data collector, Survey Solutions provides the resources and support necessary to transform data collection into a seamless and impactful experience. Embrace the future of data gathering with Survey Solutions—where precision meets efficiency.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-download.html#survey-solution-accounts",
    "href": "data-download.html#survey-solution-accounts",
    "title": "2  Data Download",
    "section": "2.2 Survey Solution Accounts",
    "text": "2.2 Survey Solution Accounts\nIn Survey Solutions, there are six main types of user accounts, each with different roles and responsibilities. Here’s a breakdown:\n\nAdministrator:\n\n\nRole: Manages the technical aspects of the Survey Solutions server.\nResponsibilities:\n\nSet up and configure the Survey Solutions server.\nManage server performance, updates, and backups.\nHandle user management (creation and deletion of accounts).\nEnsure security, including password management and system access.\nMonitor server health and log files.\n\n\n\nHeadquarters (HQ):\n\n\nRole: Manages the entire survey process, including questionnaire management, assignments, and overall data flow.\nResponsibilities:\n\nCreate and manage survey assignments.\nMonitor survey progress and interview status.\nAccess all collected data.\nAdminister users and roles.\nHandle questionnaire uploads and server management.\n\n\n\nSupervisor:\n\n\nRole: Oversees fieldwork operations, manages interviewers, and reviews their work.\nResponsibilities:\n\nReview completed interviews submitted by interviewers.\nApprove or reject interviews.\nManage interviewers and assignments within their team.\nMonitor the status of interviews and progress.\n\n\n\nInterviewer:\n\n\nRole: Conducts interviews and collects data in the field using a tablet or computer.\nResponsibilities:\n\nConduct face-to-face interviews with respondents.\nUpload collected data to the server for review.\nCommunicate with supervisors on any issues related to interviews.\n\n\n\nObserver:\n\n\nRole: Has read-only access to monitor the progress of the survey without the ability to make changes.\nResponsibilities:\n\nView interviews and their status.\nGenerate reports and monitor survey performance.\n\n\n\nAPI User:\n\n\nRole: Provides programmatic access to Survey Solutions through its API for automation and integration purposes.\nResponsibilities:\n\nFetch survey data via the API for external analysis.\nAutomate the survey workflow by integrating with other systems (e.g., data processing or visualization tools).\nCreate assignments, retrieve reports, and manage users programmatically.\n\n\nOf great importance for data management workflow is the API User. You need to talk to the Administrator, who most of the times is either system administrator or programmer, who set up and configured the Survey Solutions server to create for you an API user.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-download.html#survey-solution-api",
    "href": "data-download.html#survey-solution-api",
    "title": "2  Data Download",
    "section": "2.3 Survey Solution API",
    "text": "2.3 Survey Solution API\nSurvey Solutions includes a powerful and flexible API which allows automating some tasks and allows our users to build larger systems, which may compliment Survey Solutions to achieve larger goals. \nSome examples of use could be:\n\nschedule periodic export of collected data\nan external dashboard or monitoring and reporting system, which updates some indicators every night and publishes them to a website, or\nan external checking and validation system which verifies collected data against some external sources of information and rejects automatically the incorrect interviews, or\nan integrated system, which utilizes Survey Solutions for data collections tasks and a statistical package for continuous analysis,\nfacility management, inventory and price monitoring systems.\n\nFor the purposes of this tutorial, our focus will be on the first use case.\n\n2.3.1 API Clients\nThere are a number of API clients for Survey Solutions. They are listed below.\n\n\n\n\n\n\n\n\n\nAPI Clients\nMaintainer\nSpecific Name\nLanguage\n\n\n\n\n.NET package\nAndrii Kozhyn\nSurveySolutionsClient\nC#\n\n\nPowerShell module\nZurab Sajaia\nSSAW\nPowershell\n\n\nPython package\nZurab Sajaia\nssaw\nPython\n\n\nR package\nMichael Wild\nSurveySolutionsAPI\nR\n\n\nR package\nArthur Shaw\nsusoapi\nR\n\n\nR package\nLena Nguyen\nSuSoAPI\nR\n\n\nStata package\nSergiy Radyakin\nsusoapi\nStata\n\n\n\nFor more details about each of the clients, check here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-download.html#python-package",
    "href": "data-download.html#python-package",
    "title": "2  Data Download",
    "section": "2.4 Python Package",
    "text": "2.4 Python Package\nI use ssaw Python package as a Survey Solutions API wrapper. It is easy to use and very flexible. We’ll focus on key data management procedures, but full details are in the online documentation.\n\n2.4.1 Installation\nTo install ssaw, simply run this command in your terminal:\n\npip install ssaw\n\n\n2.4.2 Modules\nI use the following modules in data management pipeline especially when working with Survey Solutions:\n\nimport requests\nfrom ssaw import Client, ExportApi, QuestionnairesApi, models\nfrom time import sleep\nimport configparser\nimport os\n\n\n\n2.4.3 Connect to server\nTo communicate with Survey Solutions server, you first need to instantiate a client. You remember the API user you created or was created for you? It is necessary at this point. To connect to Survey Solution you need four pieces of information i.e.\n\nParameters\n\n\nurl (str) – URL of the headquarters app\napi_user (Optional[str]) – API user name\napi_password (Optional[str]) – API user password\nworkspace (str) – Name of the workspace.\n\n\n\nThere are two ways to provide these parameters.\n\nOne is to hard-code them in the script.\n\nThis method is insecure for handling sensitive data. If you push this script to a public repository, anyone can access these details. Best practices in data management require keeping sensitive information secure and out of reach of unauthorized individuals.\n\nUsing config files\nA more secure way to handle sensitive data is by using configuration files. Instead of hard-coding credentials directly in the script, you can store them in a separate config file and load them securely. There are several common formats for configuration files such as .ini files (Initialization files), .json files (JavaScript Object Notation), .yaml files (YAML Aint Markup Language), .properties files (Java-style-value pair format). I decided to use .ini because they are lightweight and easy-to-use configuration files that provide a straightforward way to store settings in a simple key-value format. They allow for comments and enable dividing the configuration into sections.\nTo prevent sensitive data from being exposed, follow these security practices:\n\nAdd Config File to .gitignore: Ensure the config file is not included in version control (e.g., GitHub) by adding it to .gitignore.\nRestrict File Permissions: Limit who can read the config file by changing its permissions so only authorized users can access it.\n\n\n\n\n2.4.4 Export Data\nThe export module contains methods to find and download an already generated data package, or trigger and manage a new generation job.\nAccessing Questionnaire Id Web Interface\nBelow are steps to access your questionnaire id:\n\n\n\n\n\nLog in into the Survey Solution and pick the right workspace\n\n\n\n\n\n\n\nClick on desired questionnaire then select details\n\n\n\n\n\n\n\nLocate the questionnaire at the address bar\n\n\n\n\nBetween the Details/ and $ sign is the id.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-download.html#download-script",
    "href": "data-download.html#download-script",
    "title": "2  Data Download",
    "section": "2.5 Download Script",
    "text": "2.5 Download Script\nThe complete script to download data is given below:\n\n# Purpose: Download the trial dataset from the Survey Solutions server\n# Author: Moses Otieno\n# Date Created: 14 Oct 2024\n# Date Modified: 14 Oct 2024\n\n# ----- Modules required\n\nimport requests\nfrom ssaw import Client, ExportApi, QuestionnairesApi\nfrom ssaw import models\nfrom time import sleep\nimport configparser\n\n\n# ---- Read and get config values \n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')\n\nurls = [config['susol']['url']]\n\nsurvey_test_id = config['susol']['survey_test_id']\nmal_id = config['susol']['mal_id']\n\n# ---- Specify the questionnaires to download data from \n\nquestionnaires = {\n    \"survey_test\": survey_test_id,\n    \"malaria\": mal_id\n}\n\n\n# ----- Define functions\n\ndef connect_to_internet(url='http://www.google.com/', timeout=5):\n    \"\"\"Check internet connectivity by pinging the given URL.\"\"\"\n    try:\n        _ = requests.head(url, timeout=timeout)\n        return True\n    except requests.ConnectionError:\n        return False\n\n\ndef connect_to_server():\n    \"\"\"Attempt to connect to the Survey Solutions server via available URLs.\"\"\"\n    for url in urls:\n        if connect_to_internet(url):\n            print(f'Connected to server: {url}')\n            return url\n    print(\"No connection to any server.\")\n    return None\n\n\ndef get_questionnaire_versions(client, questionnaire_id):\n    \"\"\"Retrieve all versions of a questionnaire.\"\"\"\n    return [(q.id, q.version) for q in QuestionnairesApi(client).get_list(questionnaire_id=questionnaire_id)]\n\n\ndef download_questionnaire_data(client, qid, version, export_path='data'):\n    \"\"\"Export and download questionnaire data in STATA format.\"\"\"\n    try:\n        export_object = models.ExportJob(qid, export_type='STATA')\n        ExportApi(client).start(export_object, wait=True)\n        ExportApi(client, workspace=\"susoltest\").get(questionnaire_identity=qid, export_type='STATA', show_progress=True, generate=True, export_path=export_path)\n        print(f'Successfully downloaded version: {version}')\n    except Exception as e:\n        print(f'Failed to download version: {version}. Error: {e}')\n\n\ndef download_all_versions(client, qnrids, qnrversions):\n    \"\"\"Loop through all versions of a questionnaire and download data.\"\"\"\n    for qnrid, version in zip(qnrids, qnrversions):\n        download_questionnaire_data(client, qnrid, version)\n\n\n# ----- Main execution logic\ndef main():\n    # Connect to the server and check internet connectivity\n    server_url = None\n    for attempt in range(1, 2):\n        print(f'Attempt {attempt} to connect...')\n        server_url = connect_to_server()\n        if server_url:\n            break\n        sleep(5)\n    if not server_url:\n        print(\"Failed to connect after 10 attempts.\")\n        return\n\n    \n\n    # Retrieve necessary configuration details\n    api_user = config['susol']['api_user']\n    passwd = config['susol']['api_password']\n    wkspace = config['susol']['workspace']\n    quizid = config['susol']['survey_test_id']\n\n   # Initialize the Survey Solutions client\n   \n    client = Client(url=server_url, api_user=api_user, api_password=passwd, workspace=wkspace)\n\n    # Process each questionnaire\n    \n    for quizname, quizid in questionnaires.items():\n        qnr_data = get_questionnaire_versions(client, quizid)\n        if qnr_data:\n            qnrids, qnrversions = zip(*qnr_data)\n            print(f'The highest version of {quizname} is {max(qnrversions)}. Downloading versions from {min(qnrversions)} to {max(qnrversions)}.')\n            download_all_versions(client, qnrids, qnrversions)\n        else:\n            print(f'No versions found for {quizname}.')\n\nif __name__ == \"__main__\":\n    main()\n\n\n2.5.1 Customizing Download Script\nYou need to provide specific details in the script to download your data:\n\nconfig.ini: Create a config.ini file with a section called susol. Include the required details: api user, api password, url, and workspace.\nQuestionnaire: Identify the IDs of the questionnaires you want to download and add them to the config file.\nexport_path: Specify the download directory for the data. The script assumes there’s a data directory in the same path; if not, create one or specify your desired directory.\n\nOnce you have this information, you’ll be able to download your data.\n\n\n2.5.2 Error Handling and Debugging\n\nIf the script fails to connect, check your internet connection and the details in config.ini.\nIf you receive an “Invalid Questionnaire ID” error, verify that the IDs in the questionnaires dictionary are correct.\nMonitor the output messages for any errors during the download process.\n\n\n\n2.5.3 Convention: Assigning One Workspace to One Directory\nIn many data management and programming workflows, particularly when dealing with multiple datasets, scripts, and configurations, establishing a clear and consistent directory structure is essential for maintaining organization and efficiency. I strongly advocate for the convention of assigning one workspace to one directory. A workspace can contain multiple questionnaires, so this script assumes that you will work with each workspace separately.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-download.html#summary",
    "href": "data-download.html#summary",
    "title": "2  Data Download",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nBelow is a summary of the steps you need to undertake in order to download data from Survey Solutions server to your machine.\n\nCreate API User: Ensure you have an API user created.\nGather Information: Collect the following:\n\nURL\nAPI username\nAPI password\nWorkspace\nQuestionnaire ID\n\nInstall ssaw Module: Use Python to install the ssaw module.\nCreate .ini Config File: Store all necessary information for the Survey Solutions Server.\nRun the Script: Execute the script to download the data!\n\nTo deepen your understanding in Python, I would recommend Automate the Boring Stuff with Python(Sweigart, Al 2019). It is a highly praised book that serves as an excellent introduction to programming for beginners, particularly those looking to leverage Python for practical tasks. For data management and analysis in Python, Python for Data Analysis by Wes McKinney (McKinney, Wes 2018) is excellent.\n\n\n\n\nMcKinney, Wes. 2018. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. 2nd ed. O’Reilly Media, Inc. https://wesmckinney.com/book/.\n\n\nSweigart, Al. 2019. Automate the Boring Stuff with Python: Practical Programming for Total Beginners. 2nd ed. No Starch Press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Download</span>"
    ]
  },
  {
    "objectID": "data-import.html",
    "href": "data-import.html",
    "title": "3  Data Import",
    "section": "",
    "text": "3.1 Introduction\nAfter downloading the data, the next step is to import it into your programming environment for management. This section on data import is essential because Survey Solutions stores the downloaded data in a specific format. In our download script, we specified export_type='STATA', which I prefer because STATA files include both variable labels and value labels, eliminating the need to redefine them. Additionally, note that the downloaded data is in a zipped format. If you have two versions of a questionnaire, you’ll receive two separate zipped files. There are two ways you can import this data into R:\nOur focus will be the second one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "data-import.html#introduction",
    "href": "data-import.html#introduction",
    "title": "3  Data Import",
    "section": "",
    "text": "Manual Import: Unzip each folder and then import the individual files into your programming environment.\nAutomated Import: Use a script to handle all the importing procedures, streamlining the process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "data-import.html#automated-import",
    "href": "data-import.html#automated-import",
    "title": "3  Data Import",
    "section": "3.2 Automated Import",
    "text": "3.2 Automated Import\nUsing a script to automate the import process is generally more efficient than manually unzipping and importing individual files. Here are some reasons why:\n\nTime-Saving: A script can process multiple files in a single run, eliminating the need for repetitive manual actions.\nReduced Errors: Automating the process minimizes the risk of human error that can occur during manual imports, such as skipping files or importing them incorrectly.\nConsistency: A script ensures that the same import process is followed every time, leading to consistent results.\nScalability: If the number of files or datasets increases, a script can easily accommodate the additional data without requiring more manual effort.\nEasy Adjustments: If changes are needed (e.g., modifying file paths or import settings), you can update the script instead of repeating the manual steps.\nAdaptability: you can use the same script for different projects with minimal changes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "data-import.html#survey-solutions-data-structure",
    "href": "data-import.html#survey-solutions-data-structure",
    "title": "3  Data Import",
    "section": "3.3 Survey Solutions Data Structure",
    "text": "3.3 Survey Solutions Data Structure\nBefore diving into data importation, it’s crucial to understand the structure of the zipped directory and its contents from Survey Solutions. This knowledge will help you navigate the files efficiently and ensure that you are importing the correct data. The naming convention is questionnaire-name_versionnumber_Exporttype_All.zip. Example ssolutions_training_1_STATA_All.zip.\n\n\n\nContents of zipped directory\n\n\nThe contents include the data and metadata:\n\n3.3.1 Metadata Files\nThese files provide additional context about the data. They include:\n\nassignment_actions\ninterview_actions\ninterview_comments\ninterview_diagnostics\ninterview_errors\nQuestionnaire: A directory containing the questionnaire\n\n\n\n3.3.2 Data Files\nThe data files are the core components of the downloaded dataset from Survey Solutions. Here’s a closer look at their significance and structure:\n\nMain Questionnaire File:\n\nThis file is named after the questionnaire itself and serves as the primary dataset containing the survey responses. The questionnaire variable is crucial here, as it uniquely identifies the survey you conducted. This unique name allows you to easily reference and manage the data throughout your analysis. In this context it is ssolutions_training\n\nRoster Datasets:\n\nIn addition to the main questionnaire file, there may be several additional files that contain data from rosters. Rosters are used in surveys to collect information on groups of similar items or respondents (e.g., household members, patients, etc.).\nEach roster file contains responses related to specific sections of the questionnaire and may include variables corresponding to individual roster entries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "data-import.html#import-data-in-r",
    "href": "data-import.html#import-data-in-r",
    "title": "3  Data Import",
    "section": "3.4 Import Data in R",
    "text": "3.4 Import Data in R\nNow that you understand the data structure and the contents of the zipped files from Survey Solutions, let’s dive into the process of importing this data into R. Importing data correctly is crucial for effective management and analysis.\nThe packages for use include:\n\nlibrary(tidyverse)          # Data management\nlibrary(haven)              # Import the stata files\n\nThe body of the code is below:\n\n# | include: false\n\n#---- List all the zip files. These are different versions of the questionnaire\n\nall_zips &lt;- list.files(\"data\", pattern = \"ssolutions_training.*.zip\",\n                       full.names = T)\n\n\n# ---- Download main questionnaire\n\nall_ssolutions_training &lt;- vector(\"list\")\n\nfor (zipfile in all_zips){\n\n  qversion &lt;- str_extract(zipfile, \"_\\\\d+_STATA_\")\n  qversion &lt;- parse_number(qversion)\n\n\n  unzip(zipfile, files = c(\"ssolutions_training.dta\"),\n        exdir = \"data\")\n\n\n  all_ssolutions_training[[zipfile]] &lt;- read_dta(\"data/ssolutions_training.dta\") |&gt;\n    mutate(quiz_version = qversion)\n}\n\n\n\nssolutions_training &lt;- all_ssolutions_training |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\n\n# ---- Save the main questionnaire\n\nwrite_rds(ssolutions_training, \"data/ssolutions_training_main.rds\")\n\n\n# ---- Download the rosters\n\nall_membre &lt;- vector(\"list\")\nall_cultureanne &lt;- vector(\"list\")\nall_r200 &lt;- vector(\"list\")\n\n\nfor (zipfile in all_zips){\n\n  qversion &lt;- str_extract(zipfile, \"_\\\\d+_STATA_\")\n  qversion &lt;- parse_number(qversion)\n\n\n  unzip(zipfile, files = c(\"membre.dta\", \"r_cultureAnnee.dta\", \"r200.dta\"),\n        exdir = \"data\")\n\n\n  all_membre[[zipfile]] &lt;- read_dta(\"data/membre.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n  all_cultureanne[[zipfile]] &lt;- read_dta(\"data/r_cultureAnnee.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n  \n  all_r200[[zipfile]] &lt;- read_dta(\"data/r200.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n}\n\n\nmembre &lt;- all_membre |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\n\ncultureanne &lt;- all_cultureanne |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\nr200 &lt;- all_r200 |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\n\n3.4.1 Full Script\n\nlibrary(tidyverse)          # Data management\nlibrary(haven)              # Import the stata files\n\n#---- List all the zip files. These are different versions of the questionnaire\n\nall_zips &lt;- list.files(\"data\", pattern = \"ssolutions_training.*.zip\",\n                       full.names = T)\n\n\n# ---- Download main questionnaire\n\nall_ssolutions_training &lt;- vector(\"list\")\n\nfor (zipfile in all_zips){\n\n  qversion &lt;- str_extract(zipfile, \"_\\\\d+_STATA_\")\n  qversion &lt;- parse_number(qversion)\n\n\n  unzip(zipfile, files = c(\"ssolutions_training.dta\"),\n        exdir = \"data\")\n\n\n  all_ssolutions_training[[zipfile]] &lt;- read_dta(\"data/ssolutions_training.dta\") |&gt;\n    mutate(quiz_version = qversion)\n}\n\n\n\nssolutions_training &lt;- all_ssolutions_training |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\n# ---- Save the main questionnaire\n\nwrite_rds(ssolutions_training, \"ssolutions_training_main.rds\")\n\n\n# ---- Download the rosters\n\nall_membre &lt;- vector(\"list\")\nall_cultureanne &lt;- vector(\"list\")\nall_r200 &lt;- vector(\"list\")\n\n\nfor (zipfile in all_zips){\n\n  qversion &lt;- str_extract(zipfile, \"_\\\\d+_STATA_\")\n  qversion &lt;- parse_number(qversion)\n\n\n  unzip(zipfile, files = c(\"membre.dta\", \"r_cultureAnnee.dta\", \"r200.dta\"),\n        exdir = \"data\")\n\n\n  all_membre[[zipfile]] &lt;- read_dta(\"data/membre.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n  all_cultureanne[[zipfile]] &lt;- read_dta(\"data/r_cultureAnnee.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n  \n  all_r200[[zipfile]] &lt;- read_dta(\"data/r200.dta\") |&gt;\n    mutate(quiz_version = qversion)\n  \n}\n\n\nmembre &lt;- all_membre |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\n\ncultureanne &lt;- all_cultureanne |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\nr200 &lt;- all_r200 |&gt; \n  map(bind_rows, .progress = TRUE) |&gt;  \n  list_rbind() \n\nwrite_rds(membre, \"data/membre.rds\")\nwrite_rds(cultureanne, \"data/cultureanne.rds\")\nwrite_rds(r200, \"data/r200.rds\")\n\n\n\n3.4.2 Optimised Code\n\nlibrary(tidyverse)          # Data management\nlibrary(haven)              # Import the Stata files\n\n# ---- List all the zip files. These are different versions of the questionnaire\nall_zips &lt;- list.files(\"data\", pattern = \"ssolutions_training.*.zip\", \n                       full.names = TRUE)\n\n# ---- Function to extract and read data\nread_data_from_zip &lt;- function(zipfile, file_name, qversion) {\n  unzip(zipfile, files = file_name, exdir = \"data\")\n  read_dta(file.path(\"data\", file_name)) %&gt;% mutate(quiz_version = qversion)\n}\n\n# ---- Download main questionnaire\nall_ssolutions_training &lt;- map(all_zips, function(zipfile) {\n  qversion &lt;- parse_number(str_extract(zipfile, \"_\\\\d+_STATA_\"))\n  read_data_from_zip(zipfile, \"ssolutions_training.dta\", qversion)\n})\n\nssolutions_training &lt;- bind_rows(all_ssolutions_training, .id = \"source\")  # Add a source column if needed\n\n# ---- Save the main questionnaire\nwrite_rds(ssolutions_training, \"data/ssolutions_training_main.rds\")\n\n# ---- Download the rosters\nall_rosters &lt;- map(all_zips, function(zipfile) {\n  qversion &lt;- parse_number(str_extract(zipfile, \"_\\\\d+_STATA_\"))\n  \n  # Read all roster files into a list\n  roster_files &lt;- c(\"membre.dta\", \"r_cultureAnnee.dta\", \"r200.dta\")\n  map(roster_files, ~ read_data_from_zip(zipfile, ., qversion))\n})\n\n# ---- Combine roster data\nmembre &lt;- bind_rows(map(all_rosters, `[[`, 1), .id = \"source\")  # First roster: membre\ncultureanne &lt;- bind_rows(map(all_rosters, `[[`, 2), .id = \"source\")  # Second roster: r_cultureAnnee\nr200 &lt;- bind_rows(map(all_rosters, `[[`, 3), .id = \"source\")  # Third roster: r200\n\n\nwrite_rds(membre, \"data/membre.rds\")\nwrite_rds(cultureanne, \"data/cultureanne.rds\")\nwrite_rds(r200, \"data/r200.rds\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "4  Data Cleaning",
    "section": "",
    "text": "4.1 Introduction\nData cleaning is a vital step in data analysis, ensuring accuracy and consistency in your dataset. While there isn’t a one-size-fits-all template, each dataset presents unique challenges that require context-specific solutions. A thorough understanding of the study design is key to effective data cleaning. For instance, cross-sectional studies should have unique participant records, whereas longitudinal studies involve repeated measures.\nAnother crucial detail is the unit of analysis. For example, in datasets involving households and their members, it’s essential to understand how households and participants are uniquely identified to ensure accurate data handling.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#introduction",
    "href": "data-cleaning.html#introduction",
    "title": "4  Data Cleaning",
    "section": "",
    "text": "4.1.1 Primary Key\nA primary key is a unique identifier for each record in a dataset, ensuring that no two rows are identical. It is crucial for maintaining data integrity, especially when dealing with relational databases or complex datasets. The primary key allows you to retrieve, update, or relate specific records without ambiguity.\nIn a dataset involving households and members, for example:\n\nHousehold ID might be the primary key for identifying households.\nMember ID (within a household) could serve as a secondary key to uniquely identify individuals in relation to the household.\n\nThe primary key is essential for linking related tables (e.g., household and member tables) and avoiding data duplication or inconsistencies. The study design should have a mechanism of uniquely identifying the records in a dataset.\n\n\n4.1.2 Survey Solutions Primary Key\nIn Survey Solutions, linking the main questionnaire dataset with roster datasets is done through two key variables:\n\ninterview_key: This is a user-friendly identifier used to uniquely reference interviews. It can be helpful when referencing or reviewing specific interviews during data collection or management.\ninterview_id: This is a unique identifier generated by the system for each interview. It serves as a reliable key to link datasets, including the main questionnaire and any associated rosters, ensuring consistency and accuracy when combining data across various sections of the study.\n\nThese variables play a critical role in maintaining the integrity of data across the different components of the survey.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#common-data-cleaning-process",
    "href": "data-cleaning.html#common-data-cleaning-process",
    "title": "4  Data Cleaning",
    "section": "4.2 Common Data Cleaning Process",
    "text": "4.2 Common Data Cleaning Process\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(haven)\n\n\n4.2.1 Cleaning Column Names\n\nsstraining_main &lt;- read_rds(\"data/ssolutions_training_main.rds\")\n\n\n\nsstraining_main &lt;- sstraining_main |&gt; \n  clean_names()\n\n\nmembers &lt;- read_rds(\"data/membre.rds\")\n\nmembers &lt;- members |&gt; \n  clean_names()\n\n\n\n4.2.2 Check Duplicates\n\ndups_ssmain &lt;- sstraining_main |&gt; \n  get_dupes(idhh)\n\n\n\n4.2.3 Handle Missing Values\n\nlibrary(naniar) \ngenerate_missing_report &lt;- function(data) {\n  missing_report &lt;- miss_var_summary(data, order = TRUE,\n                                     add_cumsum = TRUE)  |&gt; \n    filter(n_miss != 0) %&gt;%\n    dplyr::select(variable, pct_miss, n_miss)  |&gt; \n    mutate(pct_miss = round(pct_miss, 1)) |&gt; \n    filter(pct_miss &gt; 0) |&gt; \n    rename(percentage_missing = pct_miss)\n  \n  missing_report\n  \n}\n\n\ngenerate_missing_report(sstraining_main)\n\n# A tibble: 17 × 3\n   variable        percentage_missing n_miss\n   &lt;chr&gt;                        &lt;num&gt;  &lt;int&gt;\n 1 q_aepotravail_1                100      1\n 2 q_aepotravail_2                100      1\n 3 q_aepotravail_3                100      1\n 4 q_aepotravail_4                100      1\n 5 q_aepotravail_9                100      1\n 6 qd24a_1                        100      1\n 7 qd24a_2                        100      1\n 8 qd24a_3                        100      1\n 9 qd25                           100      1\n10 qd26                           100      1\n11 qd27                           100      1\n12 qd28                           100      1\n13 qd29                           100      1\n14 qd30                           100      1\n15 qd31                           100      1\n16 qd32                           100      1\n17 qd32a                          100      1\n\nsstraining_main &lt;- sstraining_main |&gt; \n  remove_empty(which = c(\"rows\", \"cols\"))\n\n\n\n4.2.4 Type Casting\nThis process involves transforming data from one type to another, such as converting a string to a number, or an integer to a floating-point number, depending on the requirements of the operation.\n\nsstraining_main &lt;- sstraining_main |&gt; \n  mutate(across(where(is.labelled), as_factor))\n\nI usually convert labeled types to factors which are useful in further analysis.\n\n\n4.2.5 Labeling Variables\nOne of the significant advantages of using Survey Solutions is the ability to label variables at the time of questionnaire design. This feature ensures that each variable in your survey is clearly identified with a meaningful label, which simplifies data management and analysis later on.\nHowever you may want to manually label the variables.\n\nattr(sstraining_main[['qa03']], 'label') &lt;- \"Pay non-family members to work on your fields\"\n\nwrite_rds(sstraining_main, \"data/sstraining_main_clean.rds\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#summary",
    "href": "data-cleaning.html#summary",
    "title": "4  Data Cleaning",
    "section": "4.3 Summary",
    "text": "4.3 Summary\nData cleaning is inherently context-specific, meaning there is no universal template that fits all scenarios. To effectively clean your data, it is essential to thoroughly understand the study design, objectives, and other relevant factors. This knowledge allows for a tailored approach that addresses the unique requirements of the dataset and the goals of the analysis.\nTo master R, I recommend R for Data Science(Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund 2023). This book introduces the tidyverse and focuses on data manipulation, visualization, and analysis through practical examples and exercises. Its clear explanations and structured approach make it suitable for both beginners and those looking to enhance their skills. Whether you’re starting out or aiming to deepen your expertise, this book is a reliable guide for your data science journey\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "report-generation.html",
    "href": "report-generation.html",
    "title": "5  Report Generation",
    "section": "",
    "text": "5.1 Introduction\nThe ultimate objective of any data preprocessing effort is to generate useful data output. Whether it’s calculating summary statistics of key variables, tracking data collection progress, building dashboards with actionable insights, developing complex models, or creating other data products, reporting is essential to communicate findings effectively.\nGenerating real-time progress reports during data collection is especially important. This allows you to catch potential issues early on, such as:\nThese insights enable immediate corrective actions, ensuring data integrity throughout the collection process.\nThe great news is that report generation can be integrated seamlessly into your data management pipeline! Additionally, before diving into serious analysis, it’s crucial to perform Exploratory Data Analysis (EDA) to thoroughly check your data. EDA helps you understand the structure, detect anomalies, and identify patterns that could impact your analysis. It ensures that your dataset is clean, reliable, and ready for deeper insights. I will provide a very basic EDA on our dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Report Generation</span>"
    ]
  },
  {
    "objectID": "report-generation.html#introduction",
    "href": "report-generation.html#introduction",
    "title": "5  Report Generation",
    "section": "",
    "text": "Missed skip logic\nIncorrect validation rules\nOther data-related inconsistencies",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Report Generation</span>"
    ]
  },
  {
    "objectID": "report-generation.html#code-to-generate-report",
    "href": "report-generation.html#code-to-generate-report",
    "title": "5  Report Generation",
    "section": "5.2 Code to Generate Report",
    "text": "5.2 Code to Generate Report\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(flextable)\n\n\nsstraining_main &lt;- read_rds(\"data/sstraining_main_clean.rds\")\nmembers &lt;- read_rds(\"data/membre.rds\")\n\nSo far we have visited 1 households and captured information on 3 members of the households. Below is a summary of household information:\n\n5.2.1 Household Information\nThe variables here start with qs or qm\n\nsstraining_main |&gt; \n  select(starts_with(c(\"qs\", \"qm\"))) |&gt; \n  select(where(is.factor) | where(is.factor)) |&gt; \n  select(1:5) |&gt; \n  tbl_summary(type = list(\n                where(is.factor) ~ \"categorical\",\n                where(is.numeric) ~ \"continuous\"), missing_text =  \"Missing\") |&gt; \n  bold_labels() |&gt; \n  modify_header(label = \"**Variable**\") |&gt; \n  modify_footnote(everything() ~ NA) |&gt; \n  gtsummary::as_flex_table() |&gt;\n  theme_box() |&gt; \n  align(i = 1, align = \"center\", part = \"header\") |&gt; \n  flextable::font(fontname = \"Calibri\", part = \"all\") |&gt; \n  flextable::padding(padding = 1, part = \"all\") |&gt; \n  bold(part = \"header\") |&gt; \n  bg(bg = \"gray\", part = \"header\")\n\nVariableN = 1RoofSheets or similar0 (0%)Straw/ thatch1 (100%)Clay0 (0%)Wood0 (0%)Other (specify)0 (0%)WallConcrete1 (100%)Chipped stone0 (0%)Clay0 (0%)Wood0 (0%)Tent0 (0%)Other (specify)0 (0%)GroundPure cement0 (0%)Clay1 (100%)Tile0 (0%)Other (specify)0 (0%)Water sourceFaucet/Tap0 (0%)Pump/drilling0 (0%)Well1 (100%)Running water0 (0%)Rain0 (0%)Bottle/bag0 (0%)Other (specify)0 (0%)Water source locationIn the own house0 (0%)In the own court/compound0 (0%)In the neighbor's court/compound1 (100%)Public well/tank/drilled pump0 (0%)stream/river0 (0%)Other (specify)0 (0%)\n\n\n\n\n5.2.2 Agriculture\nThe variables here start with qa and qd.\n\nsstraining_main |&gt; \n  select(starts_with(c(\"qa\", \"qd\"))) |&gt; \n  select(where(is.factor) | where(is.factor)) |&gt; \n  select(1:5) |&gt; \n  tbl_summary(type = list(\n                where(is.factor) ~ \"categorical\",\n                where(is.numeric) ~ \"continuous\"), missing_text =  \"Missing\") |&gt; \n  bold_labels() |&gt; \n  modify_header(label = \"**Variable**\") |&gt; \n  modify_footnote(everything() ~ NA) |&gt; \n  gtsummary::as_flex_table() |&gt;\n  theme_box() |&gt; \n  align(i = 1, align = \"center\", part = \"header\") |&gt; \n  flextable::font(fontname = \"Calibri\", part = \"all\") |&gt; \n  flextable::padding(padding = 1, part = \"all\") |&gt; \n  bold(part = \"header\") |&gt; \n  bg(bg = \"gray\", part = \"header\")\n\nVariableN = 1Have a farm where you grow cropsYes1 (100%)No0 (0%)Pay non-family members to work on your fieldsYes0 (0%)No1 (100%)Number of sicklesNone1 (100%)Don't know0 (0%)Number of large trolleysNone1 (100%)Don't know0 (0%)Number of small trolleysNone1 (100%)Don't know0 (0%)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can continue with other sections as required.\n\n\n\n\n5.2.3 Summary\nRun your report on cleaned or partially cleaned data, tailoring the content to the needs of your audience. Review the distribution of key variables, and don’t overlook free text fields, as they can provide valuable insights for categorical variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Report Generation</span>"
    ]
  },
  {
    "objectID": "master.html",
    "href": "master.html",
    "title": "6  Master Script",
    "section": "",
    "text": "6.1 Introduction\nA key way to streamline workflows in data management is by using a master script that runs other multiple scripts. This approach centralizes execution, ensuring consistency and efficiency across tasks. A master script acts as the central hub that coordinates and runs other scripts, automating various tasks and ensuring consistency across processes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Master Script</span>"
    ]
  },
  {
    "objectID": "master.html#advantages",
    "href": "master.html#advantages",
    "title": "6  Master Script",
    "section": "6.2 Advantages",
    "text": "6.2 Advantages\n\nEfficiency: It saves time by automating repetitive tasks, reducing the need for manual intervention.\nConsistency: Ensures that all tasks are performed in the same sequence and manner each time, minimizing errors.\nTime saving: Instead of running each data cleaning script individually, the master script can execute them all at once.\nScalability: You can increase the number of scripts by adding them to the master script.\n\nBy integrating all essential steps into a single execution file, a master script optimizes data management, making the entire process more reliable and seamless.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Master Script</span>"
    ]
  },
  {
    "objectID": "master.html#structure-of-the-script",
    "href": "master.html#structure-of-the-script",
    "title": "6  Master Script",
    "section": "6.3 Structure of The Script",
    "text": "6.3 Structure of The Script\nRun the script form the first to the last.\n\n#| label: Structure of script\n\n#--- Import data \n\nsource(\"scripts/01-data-import.R\")\n\n# ---- Clean data\n\nsource(\"scripts/02-data-cleaning.R\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Master Script</span>"
    ]
  },
  {
    "objectID": "send-report.html",
    "href": "send-report.html",
    "title": "7  Send Report",
    "section": "",
    "text": "7.1 Introduction\nAfter generating your well-designed reports, the next crucial step is sharing them with stakeholders. I use Python, along with Gmail, to automate the process of sending updated reports, ensuring timely delivery and minimizing manual effort. This allows for a smooth, consistent distribution of reports, saving time and reducing errors.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Send Report</span>"
    ]
  },
  {
    "objectID": "send-report.html#gmail-app-password",
    "href": "send-report.html#gmail-app-password",
    "title": "7  Send Report",
    "section": "7.2 Gmail App Password",
    "text": "7.2 Gmail App Password\nGoogle provides a secure way to configure applications with its products by allowing developers to generate app passwords. These passwords are specifically designed for use in applications, offering a secure and streamlined way to integrate with Google services, such as Gmail, without using your primary account password. This enhances security and simplifies authentication for app developers. Below are steps\n\nEnable 2-Step Verification:\n\nGo to your Google Account.\nOn the left pane click on Security.\n\nGenerate an App Password:\n\nUnder How you sign to Google, expand 2-Step Verification\nGoogle may need you to provide a password to verify it’s you\nEnable 2-Step Verification\n\n\n\n2-Step Verification\n\n\nExpand App passwords section (Bottom of the page) by clicking on that arrow\n\n\n\nClick on that arrow\n\n\nEnter the name of the app; any name (training, python)\nClick Create\n\n\n\nEnter the name of the app then click Create\n\n\n\nUse the App Password:\n\nA 16-character app password will be displayed. Copy this password.\nUse this app password in place of your regular Gmail password in the application you are configuring. In our case in Python.\n\nSave Your App Password:\n\nKeep this password secure, as it grants access to your Google account from the application. You can generate multiple app passwords if needed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Send Report</span>"
    ]
  },
  {
    "objectID": "send-report.html#sending-updates",
    "href": "send-report.html#sending-updates",
    "title": "7  Send Report",
    "section": "7.3 Sending Updates",
    "text": "7.3 Sending Updates\nBelow is a Python script for sending reports via Gmail using an app password. This automates the process of sharing reports with stakeholders:\n\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nimport configparser\nimport smtplib\nfrom os.path import basename\n\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')\n\nsender = config['email']['email_address']\napp_password = config['email']['pass_word']\n\nto_emails = [config['email']['recipient1'], config['email']['recipient2'],\nconfig['email']['recipient3']]\n\n\ndef send_mail(send_from: str, subject: str, text: str,\n              send_to: list, filess=None):\n    send_to = sender if not send_to else send_to\n\n    msg = MIMEMultipart()\n    msg['From'] = send_from\n    msg['To'] = ', '.join(send_to)\n    msg['Subject'] = subject\n\n    msg.attach(MIMEText(text))\n\n    for f in filess or []:\n        with open(f, \"rb\") as fil:\n            ext = f.split('.')[-1:]\n            attachedfile = MIMEApplication(fil.read(), _subtype=ext)\n            attachedfile.add_header(\n                'content-disposition', 'attachment', filename=basename(f))\n        msg.attach(attachedfile)\n    try:\n      server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n      server.login(sender, app_password)\n      server.sendmail(send_from, send_to, msg.as_string())\n      print(\"Email sent successfully!\")\n    except smtplib.SMTPException as e:\n      print(f\"Failed to send email: {e}\")\n    finally:\n      server.quit()\n\nYou can write the body of the email in the script as below:\n\nfiles_tosend = [\"./data-import.qmd\"]   \n    \nsend_mail(\n  send_from = sender,\n  subject = \"Test Email\",\n  text = \"Dear Moses, \\nThis is a test email.\\n\\r Kind regard,, \\n Moses\",\n  filess = files_tosend,\n  send_to = to_emails)\n\nAnother option is to store the email body in a text file, allowing you to easily update the content without modifying the script. This approach keeps your workflow organized and ensures that any changes to the message can be made quickly and cleanly, maintaining a clear separation between code and content. It’s a more efficient and scalable way to handle message updates, especially when dealing with frequent changes or multiple stakeholders.\n\nwith open('message-body.txt') as f:\n  email_body = f.read()\n  \n  \nsend_mail(\n  send_from = sender,\n  subject = \"Test Email\",\n  text = email_body,\n  filess = files_tosend,\n  send_to = to_emails)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Send Report</span>"
    ]
  },
  {
    "objectID": "send-report.html#track-the-reports-sent",
    "href": "send-report.html#track-the-reports-sent",
    "title": "7  Send Report",
    "section": "7.4 Track The Reports Sent",
    "text": "7.4 Track The Reports Sent\nMySQL is the bedrock for working with relational databases. Create a database and a table where you will be tracking your progress reports. Working the MySQL involves the four steps below:\n\nGet your database connection details\n\n\nfrom mysql.connector import MySQLConnection\nimport mysql.connector\nfrom mysql.connector import Error\nfrom datetime import date\n\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')\n\n\nhost_name = config['mysql']['host']\ndb_user = config['mysql']['user']\ndb = config['mysql']['database']\npass_word = config['mysql']['password']\n\n\nCreate a database\n\n\nfrom mysql.connector import MySQLConnection\nimport mysql.connector\nfrom mysql.connector import Error\n\ndef create_db(dname):\n    try:\n        # Establish the connection\n        connection = mysql.connector.connect(\n            host=host_name,  # e.g., 'localhost' or an IP address\n            user=db_user,\n            password=pass_word\n        )\n\n        if connection.is_connected():\n            # Create a cursor object\n            cursor = connection.cursor()\n\n            # SQL statement to create the database\n            sql_create_db = f\"CREATE DATABASE IF NOT EXISTS {dname}\"\n\n            # Execute the query\n            cursor.execute(sql_create_db)\n            print(f\"Database '{dname}' created or already exists.\")\n\n            # Commit the transaction\n            connection.commit()\n    \n    except Error as e:\n        print(f\"Error: {e}\")\n    \n    finally:\n        # Close the cursor and connection\n        if connection.is_connected():\n            cursor.close()\n            connection.close()\n            print(\"MySQL connection is closed.\")\n            \ncreate_db('training')\n\nDatabase 'training' created or already exists.\nMySQL connection is closed.\n\n\n\nCreate a table\n\n\nfrom mysql.connector import MySQLConnection\nimport mysql.connector\nfrom mysql.connector import Error\n\n\ndef create_table(table_name=\"reports_tracking\"):\n\n    try:\n      # Establish the connection\n      connection = mysql.connector.connect(\n          host=host_name,\n          user=db_user,\n          password=pass_word,\n          database=db)\n\n      if connection.is_connected():\n          # Create a cursor object\n          cursor = connection.cursor()\n\n          # SQL statement to create the table with the provided table name\n          sql_create_table = f\"\"\"\n          CREATE TABLE IF NOT EXISTS {table_name} (\n              id INT AUTO_INCREMENT PRIMARY KEY,\n              entry_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n              study VARCHAR(20)\n          );\n          \"\"\"\n\n          # Execute the query\n          cursor.execute(sql_create_table)\n          print(f\"Table '{table_name}' created or already exists.\")\n\n          # Commit the transaction\n          connection.commit()\n  \n    except Error as e:\n        print(f\"Error: {e}\")\n    \n    finally:\n        # Close the cursor and connection\n        if connection.is_connected():\n          cursor.close()\n          connection.close()\n          print(\"MySQL connection is closed.\")\n\n\ncreate_table()\n\nTable 'reports_tracking' created or already exists.\nMySQL connection is closed.\n\n\n\nInsert data into the table\n\n\nfrom mysql.connector import MySQLConnection\nimport mysql.connector\nfrom mysql.connector import Error\nfrom datetime import date\n\n\ndef insert_data(study):\n    c_date = date.today()\n    try:\n        \n        # Establish the connection\n        connection = mysql.connector.connect(\n            host=host_name,  # e.g., 'localhost' or an IP address\n            user=db_user,\n            password=pass_word,\n            database=db)     \n\n        if connection.is_connected():\n                print(\"Connected to MySQL database\")\n\n                # Create a cursor object\n                cursor = connection.cursor()\n\n                # SQL INSERT statement\n                sql_insert_query = \"\"\"INSERT INTO reports_tracking (entry_date, study)\n                                    VALUES (%s, %s)\"\"\"\n                # Values to insert\n                values_to_insert = (c_date, study)\n\n                # Execute the query\n                cursor.execute(sql_insert_query, values_to_insert)\n\n                # Commit the transaction\n                connection.commit()\n                print(\"Data inserted successfully\")\n\n    except Error as e:\n        print(f\"Error: {e}\")\n    finally:\n\n        # Close the cursor and connection\n        if connection.is_connected():\n            cursor.close()\n            connection.close()\n\n# Call the function to insert data\ninsert_data(\"Training\")\n\nConnected to MySQL database\nData inserted successfully\n\n\nTo avoid sending reports multiple times in a day, you can leverage the MySQL databases. First, fetch the data from the MySQL server and check if the report has been updated. If it hasn’t been sent that day, proceed to send it. We will update our send_mail function to ensure we track our progress reports.\n\nfrom mysql.connector import MySQLConnection\nimport mysql.connector\nfrom mysql.connector import Error\nfrom datetime import date\n\ndef fetch_data():\n  try:\n        # Establish the connection\n        connection = mysql.connector.connect(\n            host=host_name,  # e.g., 'localhost' or an IP address\n            user=db_user,\n            password=pass_word,\n            database=db)     \n\n        if connection.is_connected():\n          \n                # Create a cursor object\n                cursor = connection.cursor()\n\n                # ---- Pull the entry_date from the database\n\n                cursor.execute(\"SELECT entry_date FROM reports_tracking\")\n                \n                reportcentries = cursor.fetchall()\n                \n                entries = [j.date() for i in reportcentries for j in i]\n                \n                return entries\n\n                # Commit the transaction\n                connection.commit()\n                print(\"Data inserted successfully\")\n\n  except Error as e:\n        print(f\"Error: {e}\")\n  finally:\n        \n        # Close the cursor and connection\n        if connection.is_connected():\n            cursor.close()\n            connection.close()\n\nUpdate the send_mail function\n\ndef send_mail(send_from: str, subject: str, text: str,\n              send_to: list, filess=None):\n    send_to = sender if not send_to else send_to\n\n    msg = MIMEMultipart()\n    msg['From'] = send_from\n    msg['To'] = ', '.join(send_to)\n    msg['Subject'] = subject\n\n    msg.attach(MIMEText(text))\n\n    for f in filess or []:\n        with open(f, \"rb\") as fil:\n            ext = f.split('.')[-1:]\n            attachedfile = MIMEApplication(fil.read(), _subtype=ext)\n            attachedfile.add_header(\n                'content-disposition', 'attachment', filename=basename(f))\n        msg.attach(attachedfile)\n    try:\n      server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n      server.login(sender, app_password)\n      server.sendmail(send_from, send_to, msg.as_string())\n      insert_data(\"Training\")\n      print(\"Email sent successfully!\")\n    except smtplib.SMTPException as e:\n      print(f\"Failed to send email: {e}\")\n    finally:\n      server.quit()\n\nSend updates if not sent\n\nwith open('message-body.txt') as f:\n  email_body = f.read()\n  \nentries = fetch_data()\ndate_today = date.today()\n\nif date_today not in entries:\n  send_mail(\n    send_from = sender,\n    subject = \"Test Email\",\n    text = email_body,\n    filess = files_tosend,\n    send_to = to_emails) \nelse:\n  print(\"You have sent the updates today\")\n\nYou have sent the updates today",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Send Report</span>"
    ]
  },
  {
    "objectID": "send-report.html#summary",
    "href": "send-report.html#summary",
    "title": "7  Send Report",
    "section": "7.5 Summary",
    "text": "7.5 Summary\nOnce reports are generated, the next step is ensuring timely distribution to stakeholders. Automating this process with Python and Gmail enhances efficiency by reducing manual effort. By using a Gmail app password, which offers a secure way to integrate Gmail into applications without compromising your main account password, you ensure smooth, secure, and reliable email distribution.\nThe script provided sends reports as email attachments. It reads the sender’s credentials and recipients’ information from a configuration file (config.ini). The function send_mail handles the email composition and attachments, sending them securely via Gmail’s SMTP server. In the event of failure, the script uses a try-except block to catch errors and ensures the server connection is closed.\nAn additional recommendation is to store the email body in a separate text file for easier management, especially when frequently updating the content for various stakeholders. This keeps the script clean and maintainable.\nWe finalized the script to ensure updates are sent only if they haven’t been previously sent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Send Report</span>"
    ]
  },
  {
    "objectID": "batch.html",
    "href": "batch.html",
    "title": "8  Batch Scripting",
    "section": "",
    "text": "8.1 Introduction\nA Batch Script is text file containing lines with commands that get executed in sequence by the Microsoft command interpreter (cmd.exe). Batch scripts files have the special extension BAT (.bat) or CMD (.cmd). This type of file is recognized by the Operating System and executed through an interface (called shell) provided by a system file called the command interpreter. Batch scripting is a powerful way to automate data management workflows by allowing you to write simple scripts that can execute multiple commands. It is especially useful for handling repetitive tasks such as file organization, renaming, backups, and even scheduling automated data processing. With batch scripting, you can improve efficiency, reduce manual work, and ensure consistency in your data management processes. I created a batch script to automate the entire data management workflow.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Batch Scripting</span>"
    ]
  },
  {
    "objectID": "batch.html#why-batch-scripts",
    "href": "batch.html#why-batch-scripts",
    "title": "8  Batch Scripting",
    "section": "8.2 Why Batch Scripts",
    "text": "8.2 Why Batch Scripts\nThere are tons of reasons why I prefer batch script. A few of them are list below:\n\nAutomation: Batch scripts automate repetitive tasks, saving time and reducing manual effort.\nEfficiency: They streamline processes, allowing multiple commands to run sequentially with minimal intervention.\nConsistency: Ensure uniform execution of tasks, reducing the chances of errors.\nFlexibility: Batch scripts can be customized to handle various tasks such as backups, file management, and data processing.\nIntegration: Easily integrate with other tools or scripts to enhance data workflows.\nCost-Effective: No need for additional software; batch scripting works on most operating systems.\nSimplicity: Batch scripts are easy to write and execute, requiring minimal setup.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Batch Scripting</span>"
    ]
  },
  {
    "objectID": "batch.html#batch-script-for-automation",
    "href": "batch.html#batch-script-for-automation",
    "title": "8  Batch Scripting",
    "section": "8.3 Batch Script for Automation",
    "text": "8.3 Batch Script for Automation\nBelow is a simple example of how you can write your batch script:\n\nwith open('scripts/training-batch.bat') as f:\n  batch = f.read()\n  \nprint(batch)\n\n:: ----------------------------- Begin Header -----------------------------------\n:: Name of the batch file : training-batch.bat\n::\n:: Purpose : Automate the pulling of data from the server and run the R scripts that clean data\n::\n:: Input  :  \n::\n:: Output : Updated datasets\n::          Updated reports\n::\n::\n:: Authors : Moses Otieno\n::\n::\n:: Contact Email : mosotieno25@gmail.com\n::\n::\n:: First version : 15 October 2024\n::\n::\n:: Reviewed :\n::\n:: ----------------------------- End Header--------------------------------------\n\n\necho \"Downloading and preparing project data...\"\n\n:: Change the directory appropriately\n\nD:\ncd \"D:\\InterestingTasks\\data-mngmt\\scripts\"\n\n\n:: Run the python script to pull the data from the server\n\npython \"download_data.py\"\n\n:: Wait the download\n\ntimeout /t 3  /nobreak  \n\nRscript \"master.R\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Batch Scripting</span>"
    ]
  },
  {
    "objectID": "batch.html#automating-batch-script",
    "href": "batch.html#automating-batch-script",
    "title": "8  Batch Scripting",
    "section": "8.4 Automating Batch Script",
    "text": "8.4 Automating Batch Script\nThroughout this journey, we’ve emphasized the importance of automation in streamlining our workflows. Now that we’ve developed a robust batch script, the next step is to automate its execution.\n\nFor Windows Users: Utilize the Task Scheduler to run your batch script at specified intervals or triggers. This tool allows you to schedule tasks to run automatically, ensuring your processes are carried out without manual intervention.\nFor Linux Users: Leverage cron, a time-based job scheduler, to automate your batch script execution. You can set up cron jobs to run your scripts at designated times or intervals, enhancing efficiency and reliability.\nFor Mac Users: Use launchd, the built-in service management framework, to automate your batch script. You can create a .plist file to define when and how your script should run, allowing for seamless execution of tasks at specified times or events.\n\nBy implementing these scheduling tools, you can maintain a seamless workflow, allowing you to focus on other important tasks while your scripts run automatically. I will do a qucik run on how to schedule tasks on Windows.\n\n8.4.1 Scheduling Tasks on Windows\n\nOpen Task Scheduler:\n\nPress Windows + R to open the Run dialog.\nType taskschd.msc and hit Enter.\nYou may be requested to key in your user password. Do so.\n\nCreate a New Task:\n\nIn the Task Scheduler window, click on “Create Basic Task” in the right-hand pane.\n\n\n\nClick Create Basic Task\n\n\n\nName Your Task:\n\nEnter a name and description for your task, then click “Next.”\n\n\n\nName and description of task\n\n\n\nChoose a Trigger:\n\nSelect when you want the task to start (e.g., Daily, Weekly, One time, etc.) and click “Next.”\nConfigure the trigger details (e.g., time, frequency) and click “Next.”\n\nChoose an Action:\n\nSelect “Start a program” and click “Next.”\n\nSelect Your Batch Script:\n\nClick “Browse” to locate your batch script file.\nOptionally, you can add arguments or specify the “Start in” directory.\nClick “Next.”\n\nReview and Finish:\n\nReview your task settings. If everything looks good, click “Finish.”\n\n\n\nSummary of task scheduled\n\n\n\nManage Your Task:\n\nTo edit or manage your scheduled task, locate it in the Task Scheduler Library. Right-click on the task for options like Run, End, or Delete.\n\n\n\n\n\nThe task is running!\n\n\nVoila! We have done it!!!!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Batch Scripting</span>"
    ]
  },
  {
    "objectID": "batch.html#summary",
    "href": "batch.html#summary",
    "title": "8  Batch Scripting",
    "section": "8.5 Summary",
    "text": "8.5 Summary\nIn this section, we explored the significance of batch scripting in creating an efficient data management workflow. Batch scripts automate repetitive tasks, allowing users to execute multiple commands without manual intervention, thus enhancing productivity and minimizing errors. We discussed various reasons for preferring batch scripts, including their simplicity, efficiency, and ability to manage complex workflows seamlessly.\nWe also highlighted the importance of task schedulers for automating batch scripts, with specific instructions for Windows users to schedule tasks effectively. For Linux users, we mentioned using cron jobs, and for Mac users, we suggested utilizing Automator or the launchd system for similar task automation.\nBy integrating batch scripts and task schedulers into our data management workflow, we streamline processes, improve consistency, and ultimately crown the workflow with a robust and automated solution. For more details about batch scripting check here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Batch Scripting</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Briney, K. n.d. Data Management for Researchers: Organize, Maintain\nand Share Your Data for Research Success. Pelagic Publishing.\n\n\nMcKinney, Wes. 2018. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython. 2nd ed. O’Reilly Media, Inc. https://wesmckinney.com/book/.\n\n\nSweigart, Al. 2019. Automate the Boring Stuff with Python: Practical\nProgramming for Total Beginners. 2nd ed. No Starch Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed.",
    "crumbs": [
      "References"
    ]
  }
]